IOMiner scripts for analyzing parallel I/O logs

- Parsers for parsing Darshan data and Slurm job log data
- Analyzers for analyzing Darshan, LMT, and Slurm job log data

Code structure:
---------------
-iominer 
	- data_prep
		- analyze_darshan.py
			-- parse darshan logs, and construct a big table that contains all the counters (e.g. total_POSIX_BYTES_WRITTEN) of darshan logs generated during [start time, end time]. Each record in the table records the counters of one log. 
			-- extract node count of each job from SLURM, and inline it into its corresponding record in the Darshan bigtable.
		- miner_stat.py
			-- utility functions used to construct big table
		- construct_low_bw.py
			-- utility functions used to construct big table
		- bitmap.py
			-- use bitmap to store the OSTs used by each job
		- extract_ost.py
			-- extract the metadata server utilization, OSS utilization, and OST traffic for each job based on LMT logs, and store them into lmt table
		- construct_fs_dict.py
			-- convert the lmt table into dictionary format, the key is jobID 
		- combine_logs.py
			-- combine the lmt table with the Darshan big table based the job ID.
	- analyzers
		- bw_analysis
			- heatmap.py
				-- generate heatmap for selected IO contributing factors
			- extract_contri_arg.py
				-- calculate the value of contributing factors for all the logs belonging to the same applications, the result of this script is the input of app_analyze.py
			- batch_sweep_analyze_mul.py
				-- utility for extract_contri_arg.py
			- app_analyze.py
				-- generate the data source for drawing the parallel coorindate plot of each application	
		- statistics
			- statistics.py
				-- calculate some common statistics, such as finding the top N IO-intensive applications, number of jobs using different I/O techniques (e.g. POSIX, MPI-IO, etc)
	- sweep_analysis
			- batch_sweep_analysis_2.ipynb
				-- visualize the I/O activities and the sweep-line of each job
				-- plot the IO size distribution of different ranks and OSTs in a job
				-- plot the IO request count distribution of different ranks and OSTs in a job
			
	- plot
			- ccgrid_plot
				-- scripts that generate the plots for the CCGRID paper
			- cluster_plot
				-- scripts that generate the plots for the CLUSTER paper
	- spark_test
			- analyze_darshan_spark_test.py
				-- test the performance of pyspark
			- Skeletons for implementing pyspark based IOMiner
	- slurm
			- collect_slurm_month.py
				-- generate the SLURM log during a given period [start_time, end_time]
			- slurm_stat.py
				-- APIs for calculating the SLURM CPU cycle, and constructing SLURM table
 	- lmt		
			- lmt_stat.py
				-- APIs for calculating the LMT IO coverage

Instructions to use IOMiner on Cori @ NERSC:
========================================================================

Login to Cori

module load cray-hdf5
module load python/2.7-anaconda
module load darshan
module load spark/2.0.0

1. Data preparation
	- python analyze_darshan.py
		-- copy darshan files to specified directory
		-- parse Darshan files into text format
		-- construct a big table from the parsed Darshan log
		-- The above functions can be selectively enabled following the comments in this file
	- python extract_ost.py
		-- construct a table that records the LMT information for each job.
	- python construct_fs_dict.py
		--reformat the LMT table constructed by extract_ost.py into dictionary, with JobID as the key.
	- python combine_logs.py
		--combine lmt record constructed by construct_fs_dict.py with the Darshan table constructed by analyze_darshan.py into a new table. Also extract the jobs that run longer than 5 minutes, using > 1 processes.

2. Analysis
	- python heatmap.py
		-- generate the heatmap based on the correlation matrix (resultDict.log) built from get_bw_matrix.py; 
		-- extract the top CPU cycle consumer applications, and their bandwidth distribution (the output files are used as input for script (app_cnt_rename.ipynb, app_box.ipynb) under plot/ directory)
	- python extract_contri_arg.py <application name> <number of threads>
		-- extract the I/O contributing factors of all the jobs for a given application. The result is used by app_analyze.py for generating the data source for building this application's parallel coordinate plot.
	- python app_analyze.py
		-- generate the data source of the parallel coordinate plot for a given application. The output is used as the input of the sripts (root_analysis_<app_name>.py) under plot/ directory
	- python statistics.py
		-- extract useful statistics from Darshan table constructed in data prepration (analyze_darshan.py), uncomment the script to enable each statistics. The produced "pkl" file can be used as the input for plot scripts under plot/

3. plot (.ipynb is open by jupyter-notebook)
	- ccgrid_plot
		- app_box.ipynb
			-- plot the bandwidth distribution of the top CPU cycle consumer applications, accept the output of heatmap.py as input
		- app_cnt_rename.ipynb
			-- plot the job count and CPU cycles of the top CPU cycle consumer applications, accept the output of heatmap.py as input
		- python root_analysis_<app_name>.py
			-- plot the parallel coordinate plot for a given application, accept the output of app_analyze.py as the input 
	- cluster_plot
		- io_type.ipynb
			-- plot the distribution of jobs using different I/O techniques (e.g., MPI-IO, POSIX), accept output of statistics.py as input
		- plot_top_read_app.ipynb/plot_top_write_app.ipynb
			-- plot the total IO size of top read-intensive/write-intensive applications, accept the output of statistics.py as input
		- seq_io.ipynb
			-- plot the sequential IO ratio distribution, accept the output of statistics.py as input
		- read_write_ratio.ipynb
			-- plot the distribution of read/write ratio, accept the output of statistics.py as input

4. spark_test
	- python convert_to_spark_table.py 
		-- Split the constructed bigtable into multiple logs for Spark based parallel analysis
	- analyze_darshan_spark_test.py
		-- test the performance of pyspark by executing a query that extracts the number of jobs using custom stripe configuration
			salloc -C haswell -N 4 -p debug --mem=122GB -t 00:30:00 -L SCRATCH
			module load cray-hdf5
			module load python/2.7-anaconda
			module load darshan
			module load spark/2.0.0
			start-all.sh
			pyspark
			execfile('analyze_darshan_spark_test.py')
			
	- spark_framework.py
		-- define the APIs and the test the APIs of a Spark based data analytics framework.

5. sweepline_analysis
	- batch_sweep_analysis_2.ipynb 
		-- visualize a job's I/O activities and also highlight the sweepline in the figure
		-- plot the IO size distribution of different ranks and OSTs in a job
		-- plot the IO request count distribution of different ranks and OSTs in a job
6. slurm
	- collect_slurm_month.py
		-- extract SLURM data between start_time and end_time
	- python slurm_stat.py
		-- format slurm data into table format, which is used as the input of analyze_darshan.py that inline the slurm information with Darshan records; function get_utilization_dist also plot the CPU coverage of SLURM logs produced during start_time and end_time
7. lmt
	- python lmt_stat.py
		-- get the IO coverage on the file system	
 
