IOMiner scripts for analyzing parallel I/O logs

- Parsers for parsing Darshan data and Slurm job log data
- Analyzers for analyzing Darshan, LMT, and Slurm job log data

Code structure:
---------------
-iominer 
	- data_prep
		- analyze_darshan.py
			-- parse darshan logs, and construct a big table that contains all the counters (e.g. total_POSIX_BYTES_WRITTEN) of darshan logs generated during [start time, end time]. Each record in the table records the counters of one log. 
			-- extract node count of each job from SLURM, and inline it into its corresponding record in the Darshan bigtable.
		- miner_stat.py
			-- utility functions used to construct Darshan big table
		- construct_low_bw.py
			-- utility functions used to construct Darshan big table
		- bitmap.py
			-- use bitmap to store the OSTs used by each job
		- extract_ost.py
			-- extract the metadata server utilization, OSS utilization, and OST traffic for each job based on LMT logs, and store them into lmt table
		- construct_fs_dict.py
			-- convert the lmt table into dictionary format, the key is jobID 
		- combine_logs.py
			-- combine the lmt table with the Darshan big table based the job ID.
	- analyzers
		- bw_analysis
			- heatmap.py
				-- generate heatmap for selected IO contributing factors
			- extract_contri_arg.py
				-- calculate the value of contributing factors for all the logs belonging to the same applications, the result of this script is the input of app_analyze.py
			- batch_sweep_analyze_mul.py
				-- utility for extract_contri_arg.py
			- app_analyze.py
				-- generate the data source for drawing the parallel coordinate plot of each application	
		- statistics
			- statistics.py
				-- calculate some common statistics, such as finding the top N IO-intensive applications, number of jobs using different I/O techniques (e.g. POSIX, MPI-IO, etc)
	- sweep_analysis
			- batch_sweep_analysis_2.ipynb
				-- visualize the I/O activities and the sweep-line of each job
				-- plot the IO size distribution of different ranks and OSTs in a job
				-- plot the IO request count distribution of different ranks and OSTs in a job
			
	- plot
			- ccgrid_plot
				-- scripts that generate the plots for the CCGRID paper. For example, top CPU cycle consumer applications, the boxplot about their bandwidth distribution, as well as the heatmap showing the impact of different IO performance contributing factors.
			- cluster_plot
				-- scripts that generate the plots for the CLUSTER paper. Such as finding the data size top IO intensive applications, number of jobs using MPI-IO, POSIX, etc.
	- spark_test
			- python convert_to_spark_table.py
				-- split the Darshan bigtable into multiple logs for parallel analysis of Spark 
			- analyze_darshan_spark_test.py
				-- test the performance of pyspark
			- Skeletons for implementing pyspark based IOMiner
	- slurm
			- collect_slurm_month.py
				-- generate the SLURM log during a given period [start_time, end_time]
			- slurm_stat.py
				-- APIs for calculating the SLURM CPU cycle, and constructing SLURM table
 	- lmt		
			- lmt_stat.py
				-- APIs for calculating the LMT IO coverage

Instructions to use IOMiner on Cori @ NERSC:
========================================================================

Login to Cori

module load cray-hdf5
module load python/2.7-anaconda
module load darshan
module load spark/2.0.0

modify miner_para.conf to replace "miner1217" with your iominer directory  
1. Data preparation
	- python analyze_darshan.py (output: bigtable_formated_tot_stat_adv.pkl.log)
		-- copy darshan files to specified directory
		-- parse Darshan files into text format
		-- construct a big table from the parsed Darshan log
		-- The above functions can be selectively enabled following the comments in this file
	- python extract_ost.py (output: lmt_<month>_<day>.log)
		-- construct a table that records the LMT information for each job.
	- python construct_fs_dict.py (output: lustre_info.log)
		--reformat the LMT table constructed by extract_ost.py into dictionary, with JobID as the key.
	- python combine_logs.py
		--combine lmt record constructed by construct_fs_dict.py with the Darshan table constructed by analyze_darshan.py into a new table (output:bigtable_formatted_tot_wlmt_complete.log). Also filter the jobs that satisfy given criteria (e.g., run longer than 5 minutes, using > 1 processes). Filtered jobs are stored in meaningful_job.log. The output file fs_dict.log stores the job id, start time, end time and lmt information.

2. Analysis
	- python heatmap.py
		-- generate the heatmap based on the correlation matrix (resultDict.log) built from get_bw_matrix.py; 
		-- extract the top CPU cycle consumer applications, and their bandwidth distribution (the output files app_cnt.log app_bw_ext.log are used as input for script app_cnt_rename.ipynb, app_box.ipynb under plot/ directory)
	- python extract_contri_arg.py <application name> <number of threads used to parse the logs of the same application>
		-- extract the I/O contributing factors of all the jobs for a given application, it uses the output from combine_logs.py (fs_dict.log) as the input. The output of this script is used by app_analyze.py for generating the data source for building this application's parallel coordinate plot.
	- python app_analyze.py <applicatio name>
		-- generate the data source of the parallel coordinate plot for a given application. The output is used as the input of the sripts (root_analysis_<app_name>.py) under plot/ directory
	- python statistics.py
		-- extract useful statistics from Darshan table constructed in data prepration (analyze_darshan.py), uncomment the script to enable each statistics. The produced "pkl" file can be used as the input for plot scripts under plot/

3. plot (.ipynb is open by jupyter-notebook)
	- ccgrid_plot
		- app_box.ipynb
			-- plot the bandwidth distribution of the top CPU cycle consumer applications, accept the output of heatmap.py (app_cnt.log) as input
		- app_cnt_rename.ipynb
			-- plot the job count and CPU cycles of the top CPU cycle consumer applications, accept the output of heatmap.py (app_cnt.log and app_bw_ext.log) as input
		- python root_analysis_<app_name>.py
			-- plot the parallel coordinate plot for a given application, accept the output of app_analyze.py as the input (<app_name>_plot_factors.log)
	- cluster_plot
		- io_type.ipynb
			-- plot the distribution of jobs using different I/O techniques (e.g., MPI-IO, POSIX), accept output of statistics.py as input
		- plot_top_read_app.ipynb/plot_top_write_app.ipynb
			-- plot the total IO size of top read-intensive/write-intensive applications, accept the output of statistics.py as input
		- seq_io.ipynb
			-- plot the sequential IO ratio distribution, accept the output of statistics.py as input
		- read_write_ratio.ipynb
			-- plot the distribution of read/write ratio, accept the output of statistics.py as input

4. spark_test
	- python convert_to_spark_table.py 
		-- Split the constructed bigtable into multiple logs for Spark based parallel analysis
	- analyze_darshan_spark_test.py
		-- test the performance of pyspark by executing a query that extracts the number of jobs using custom stripe configuration
			salloc -C haswell -N 4 -p debug --mem=122GB -t 00:30:00 -L SCRATCH
			module load cray-hdf5
			module load python/2.7-anaconda
			module load darshan
			module load spark/2.0.0
			start-all.sh
			pyspark
			execfile('analyze_darshan_spark_test.py')
			
	- spark_framework.py
		-- define the APIs and the test the APIs of a Spark based data analytics framework.

5. sweepline_analysis
	- batch_sweep_analysis_2.ipynb 
		-- visualize a job's I/O activities and also highlight the sweepline in the figure
		-- plot the IO size distribution of different ranks and OSTs in a job
		-- plot the IO request count distribution of different ranks and OSTs in a job
6. slurm
	- collect_slurm_month.py
		-- extract SLURM data between start_time and end_time
	- python slurm_stat.py
		-- format slurm data into table format, which is used as the input of analyze_darshan.py that inline the slurm information with Darshan records; function get_cpu_hours plots the CPU coverage of SLURM logs produced during start_time and end_time
7. lmt
	- python lmt_stat.py
		-- get the IO coverage on the file system	
 
